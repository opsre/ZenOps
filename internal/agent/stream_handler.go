package agent

import (
	"context"
	"fmt"
	"strings"

	"cnb.cool/zhiqiangwang/pkg/logx"
	"github.com/cloudwego/eino-ext/components/model/openai"
	"github.com/cloudwego/eino/components/model"
	"github.com/cloudwego/eino/schema"
	"github.com/eryajf/zenops/internal/knowledge"
	"github.com/eryajf/zenops/internal/memory"
	"github.com/eryajf/zenops/internal/service"
)

// StreamHandler æµå¼å¯¹è¯å¤„ç†å™¨
type StreamHandler struct {
	orchestrator        *Orchestrator
	fallbackModelConfig ModelConfig // å›é€€é…ç½®ï¼ˆä» config.yamlï¼‰
	tools               []schema.ToolInfo
}

// NewStreamHandler åˆ›å»ºæµå¼å¤„ç†å™¨
func NewStreamHandler(orchestrator *Orchestrator, fallbackModelConfig ModelConfig) (*StreamHandler, error) {
	return &StreamHandler{
		orchestrator:        orchestrator,
		fallbackModelConfig: fallbackModelConfig,
	}, nil
}

// getLatestModelConfig è·å–æœ€æ–°çš„ LLM é…ç½®ï¼ˆä¼˜å…ˆæ•°æ®åº“ï¼Œå›é€€åˆ° config.yamlï¼‰
func (s *StreamHandler) getLatestModelConfig(ctx context.Context) ModelConfig {
	// å°è¯•ä»æ•°æ®åº“è¯»å–é…ç½®
	configService := service.NewConfigService()
	dbLLMConfig, err := configService.GetDefaultLLMConfig()

	if err == nil && dbLLMConfig != nil && dbLLMConfig.Enabled {
		logx.Debug("Using LLM config from database: provider=%s, model=%s",
			dbLLMConfig.Provider, dbLLMConfig.Model)
		return ModelConfig{
			Model:   dbLLMConfig.Model,
			APIKey:  dbLLMConfig.APIKey,
			BaseURL: dbLLMConfig.BaseURL,
		}
	}

	// å›é€€åˆ° config.yaml
	logx.Debug("Using fallback LLM config from config.yaml")
	return s.fallbackModelConfig
}

// ChatStream æµå¼å¯¹è¯ï¼ˆå…¼å®¹ç°æœ‰æ¥å£ï¼‰
func (s *StreamHandler) ChatStream(ctx context.Context, req *ChatRequest) (<-chan string, error) {
	responseCh := make(chan string, 100)

	go func() {
		defer close(responseCh)

		// 1. æ£€æŸ¥è¯­ä¹‰ç¼“å­˜ï¼ˆä¼˜å…ˆï¼‰
		if cachedAnswer, hit, err := s.orchestrator.memoryMgr.GetSemanticCachedAnswer(ctx, req.Username, req.Message); err == nil && hit {
			logx.Info("âœ… Semantic cache hit, returning cached answer")
			responseCh <- cachedAnswer
			return
		}

		// 2. æ£€æŸ¥ç²¾ç¡®åŒ¹é…ç¼“å­˜
		cachedAnswer, hit, err := s.orchestrator.memoryMgr.GetCachedAnswer(req.Username, req.Message)
		if err == nil && hit {
			logx.Info("âœ… Exact cache hit, returning cached answer")
			responseCh <- cachedAnswer
			return
		}

		// 3. åŠ è½½å¯¹è¯å†å²
		chatLogs, err := s.orchestrator.memoryMgr.GetConversationHistory(req.ConversationID, 10)
		if err != nil {
			logx.Warn("Failed to load conversation history: %v", err)
		}

		// è½¬æ¢ä¸º memory.Message æ ¼å¼
		var history []memory.Message
		for _, log := range chatLogs {
			history = append(history, memory.Message{
				Role:      s.chatTypeToRole(log.ChatType),
				Content:   log.Content,
				CreatedAt: log.CreatedAt,
			})
		}

		// 4. åŠ è½½ç”¨æˆ·ä¸Šä¸‹æ–‡
		userCtx, err := s.orchestrator.memoryMgr.GetUserContext(req.Username)
		if err != nil {
			logx.Warn("Failed to load user context: %v", err)
		}

		// 5. æ£€ç´¢çŸ¥è¯†åº“
		var knowledgeDocs []*knowledge.Document
		if s.orchestrator.knowledgeRet != nil {
			knowledgeDocs, err = s.orchestrator.knowledgeRet.Retrieve(ctx, req.Message)
			if err != nil {
				logx.Warn("Failed to retrieve knowledge: %v", err)
			}
		}

		// 6. æ„å»º MCP å·¥å…·åˆ—è¡¨
		tools, err := s.buildMCPToolInfos(req.Username)
		if err != nil {
			logx.Warn("Failed to build MCP tools: %v", err)
			tools = nil
		}
		s.tools = tools

		// 7. æ„å»ºæ¶ˆæ¯
		messages := s.buildMessages(history, userCtx, knowledgeDocs, req.Message)

		// 8. æ‰§è¡Œæ¨ç†å¾ªç¯ï¼ˆæ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨ï¼‰
		fullResponse := s.executeLLMWithTools(ctx, messages, req.Username, responseCh)

		// 9. ä¿å­˜æ¶ˆæ¯åˆ°å†å²
		if err := s.orchestrator.memoryMgr.SaveMessage(req.ConversationID, 1, req.Message, req.Username); err != nil {
			logx.Warn("Failed to save user message: %v", err)
		}
		if err := s.orchestrator.memoryMgr.SaveMessage(req.ConversationID, 2, fullResponse, req.Username); err != nil {
			logx.Warn("Failed to save assistant message: %v", err)
		}

		// 10. æ›´æ–° QA ç¼“å­˜ï¼ˆåŒ…å«è¯­ä¹‰å‘é‡ï¼‰
		if err := s.orchestrator.memoryMgr.UpdateQACache(ctx, req.Username, req.Message, fullResponse); err != nil {
			logx.Warn("Failed to update QA cache: %v", err)
		}
	}()

	return responseCh, nil
}

// executeLLMWithTools æ‰§è¡Œ LLM æ¨ç†ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
func (s *StreamHandler) executeLLMWithTools(
	ctx context.Context,
	messages []*schema.Message,
	username string,
	responseCh chan<- string,
) string {
	// ğŸ”„ åŠ¨æ€è·å–æœ€æ–° LLM é…ç½®
	modelConfig := s.getLatestModelConfig(ctx)

	// åˆ›å»º ChatModel
	chatModel, err := openai.NewChatModel(ctx, &openai.ChatModelConfig{
		Model:   modelConfig.Model,
		APIKey:  modelConfig.APIKey,
		BaseURL: modelConfig.BaseURL,
	})
	if err != nil {
		errMsg := fmt.Sprintf("âŒ Failed to create chat model: %v", err)
		responseCh <- errMsg
		logx.Error(errMsg)
		return errMsg
	}

	var fullResponse strings.Builder
	maxIterations := s.orchestrator.maxIterations

	for i := 0; i < maxIterations; i++ {
		logx.Debug("ğŸ”„ Iteration %d/%d", i+1, maxIterations)

		// æ„å»ºè¯·æ±‚é€‰é¡¹
		opts := []model.Option{
			model.WithTemperature(0.7),
		}

		// æ·»åŠ å·¥å…·ï¼ˆå¦‚æœæœ‰ï¼‰
		if len(s.tools) > 0 {
			// è½¬æ¢ä¸º []*schema.ToolInfo
			var toolPtrs []*schema.ToolInfo
			for i := range s.tools {
				toolPtrs = append(toolPtrs, &s.tools[i])
			}
			opts = append(opts, model.WithTools(toolPtrs))
		}

		// è°ƒç”¨ ChatModel (æµå¼)
		streamReader, err := chatModel.Stream(ctx, messages, opts...)
		if err != nil {
			errMsg := fmt.Sprintf("âŒ LLM è°ƒç”¨å¤±è´¥: %v", err)
			responseCh <- errMsg
			logx.Error(errMsg)
			return errMsg
		}

		// å¤„ç†æµå¼å“åº”
		var currentContent strings.Builder
		var toolCalls []schema.ToolCall

		for {
			chunk, err := streamReader.Recv()
			if err != nil {
				break // æµç»“æŸ
			}

			// æµå¼è¾“å‡ºå†…å®¹
			if chunk.Content != "" {
				currentContent.WriteString(chunk.Content)
				fullResponse.WriteString(chunk.Content)
				responseCh <- chunk.Content
			}

			// æ”¶é›†å·¥å…·è°ƒç”¨
			if len(chunk.ToolCalls) > 0 {
				toolCalls = append(toolCalls, chunk.ToolCalls...)
			}
		}

		// æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
		if len(toolCalls) == 0 {
			// æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œå¯¹è¯ç»“æŸ
			logx.Info("âœ… LLM response completed without tool calls")
			break
		}

		// å¤„ç†å·¥å…·è°ƒç”¨
		logx.Info("ğŸ”§ Executing %d tool calls...", len(toolCalls))
		responseCh <- "\n\n"

		// æ·»åŠ  assistant æ¶ˆæ¯åˆ°å†å²
		messages = append(messages, &schema.Message{
			Role:      schema.Assistant,
			Content:   currentContent.String(),
			ToolCalls: toolCalls,
		})

		// æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
		for _, toolCall := range toolCalls {
			responseCh <- fmt.Sprintf("> ğŸ”§ è°ƒç”¨å·¥å…·: **%s**\n", toolCall.Function.Name)

			toolResult, err := s.executeToolCall(ctx, &toolCall, username)
			if err != nil {
				errMsg := fmt.Sprintf("âŒ å·¥å…·è°ƒç”¨å¤±è´¥: %v\n\n", err)
				responseCh <- errMsg
				toolResult = errMsg
			} else {
				responseCh <- "âœ… å·¥å…·æ‰§è¡Œå®Œæˆ\n\n"
			}

			// æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯å†å²
			messages = append(messages, &schema.Message{
				Role:       schema.Tool,
				Content:    toolResult,
				ToolCallID: toolCall.ID,
				Name:       toolCall.Function.Name,
			})
		}
	}

	if len(fullResponse.String()) == 0 {
		return "âš ï¸ è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°é™åˆ¶"
	}

	return fullResponse.String()
}

// executeToolCall æ‰§è¡Œå·¥å…·è°ƒç”¨
func (s *StreamHandler) executeToolCall(ctx context.Context, toolCall *schema.ToolCall, username string) (string, error) {
	// æŸ¥æ‰¾å¯¹åº”çš„ MCP Tool Adapter
	adapter := NewMCPToolAdapter(
		toolCall.Function.Name,
		"",
		nil,
		s.orchestrator.mcpServer,
		username,
	)

	// æ‰§è¡Œå·¥å…·
	result, err := adapter.InvokableRun(ctx, toolCall.Function.Arguments)
	if err != nil {
		return "", fmt.Errorf("tool execution failed: %w", err)
	}

	return result, nil
}

// buildMessages æ„å»ºæ¶ˆæ¯åˆ—è¡¨
func (s *StreamHandler) buildMessages(
	history []memory.Message,
	userCtx *memory.UserContext,
	knowledgeDocs []*knowledge.Document,
	userMessage string,
) []*schema.Message {
	var messages []*schema.Message

	// System prompt
	systemPrompt := s.orchestrator.buildSystemPrompt(userCtx, knowledgeDocs)
	messages = append(messages, &schema.Message{
		Role:    schema.System,
		Content: systemPrompt,
	})

	// å†å²æ¶ˆæ¯
	for _, msg := range history {
		messages = append(messages, &schema.Message{
			Role:    s.roleStringToEnum(msg.Role),
			Content: msg.Content,
		})
	}

	// ç”¨æˆ·æ¶ˆæ¯
	messages = append(messages, &schema.Message{
		Role:    schema.User,
		Content: userMessage,
	})

	return messages
}

// roleStringToEnum å°†å­—ç¬¦ä¸² role è½¬æ¢ä¸º Eino schema.RoleType
func (s *StreamHandler) roleStringToEnum(role string) schema.RoleType {
	switch role {
	case "user":
		return schema.User
	case "assistant":
		return schema.Assistant
	case "system":
		return schema.System
	case "tool":
		return schema.Tool
	default:
		return schema.User
	}
}

// buildMCPToolInfos æ„å»º MCP å·¥å…·ä¿¡æ¯åˆ—è¡¨
func (s *StreamHandler) buildMCPToolInfos(username string) ([]schema.ToolInfo, error) {
	// è·å–å¯ç”¨çš„ MCP å·¥å…·åˆ—è¡¨
	toolList, err := s.orchestrator.mcpServer.ListEnabledTools(context.Background())
	if err != nil {
		return nil, fmt.Errorf("failed to list enabled MCP tools: %w", err)
	}

	var tools []schema.ToolInfo
	for _, tool := range toolList.Tools {
		// æ„å»º ToolInfoï¼ˆæš‚æ—¶ä¸è®¾ç½® ParamsOneOfï¼Œå› ä¸ºç±»å‹ä¸åŒ¹é…ï¼‰
		// TODO: å®ç° MCP InputSchema åˆ° Eino ParamsOneOf çš„è½¬æ¢
		info := schema.ToolInfo{
			Name: tool.Name,
			Desc: tool.Description,
			// ParamsOneOf: éœ€è¦ç±»å‹è½¬æ¢
		}

		tools = append(tools, info)
		logx.Debug("ğŸ“¦ Loaded MCP tool: %s", tool.Name)
	}

	logx.Info("âœ… Loaded %d enabled MCP tools for stream handler", len(tools))
	return tools, nil
}

// chatTypeToRole å°† ChatType è½¬æ¢ä¸º Role å­—ç¬¦ä¸²
func (s *StreamHandler) chatTypeToRole(chatType int) string {
	switch chatType {
	case 1:
		return "user"
	case 2:
		return "assistant"
	default:
		return "system"
	}
}

// ModelConfig LLM æ¨¡å‹é…ç½®
type ModelConfig struct {
	Model   string
	APIKey  string
	BaseURL string
}
