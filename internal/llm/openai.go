package llm

import (
	"context"
	"crypto/tls"
	"errors"
	"fmt"
	"io"
	"net/http"
	"net/url"
	"sort"
	"time"

	"cnb.cool/zhiqiangwang/pkg/logx"
	openai "github.com/sashabaranov/go-openai"
)

// OpenAIClient OpenAI å…¼å®¹çš„å®¢æˆ·ç«¯
type OpenAIClient struct {
	config *Config
	client *openai.Client
}

// NewOpenAIClient åˆ›å»ºæ–°çš„ OpenAI å®¢æˆ·ç«¯
func NewOpenAIClient(config *Config) *OpenAIClient {
	clientConfig := openai.DefaultConfig(config.APIKey)

	// é…ç½® BaseURL
	if config.BaseURL != "" {
		// ç›´æ¥ä½¿ç”¨é…ç½®çš„ BaseURL,ä¸è‡ªåŠ¨æ·»åŠ  /v1
		// å› ä¸ºä¸åŒçš„ API æä¾›å•†å¯èƒ½æœ‰ä¸åŒçš„è·¯å¾„æ ¼å¼
		// ä¾‹å¦‚:OpenAI ä½¿ç”¨ /v1,æ™ºæ™® AI ä½¿ç”¨ /api/paas/v4
		clientConfig.BaseURL = config.BaseURL
		logx.Debug("OpenAI client BaseURL: %s", config.BaseURL)
	}

	// é…ç½® HTTP å®¢æˆ·ç«¯ - å‚è€ƒ chatgpt-dingtalk çš„å®ç°
	// å…³é”®:ç¦ç”¨ HTTP/2,å¼ºåˆ¶ä½¿ç”¨ HTTP/1.1 ä»¥é¿å… INTERNAL_ERROR
	transport := &http.Transport{
		MaxIdleConns:        100,
		MaxIdleConnsPerHost: 10,
		IdleConnTimeout:     90 * time.Second,
		// ç¦ç”¨ HTTP/2 - è®¾ç½®ç©ºçš„ TLSNextProto map ä¼šé˜»æ­¢ HTTP/2
		TLSNextProto: make(map[string]func(authority string, c *tls.Conn) http.RoundTripper),
	}

	clientConfig.HTTPClient = &http.Client{
		Transport: transport,
		Timeout:   600 * time.Second,
	}

	client := openai.NewClientWithConfig(clientConfig)

	logx.Info("OpenAI client initialized, model %s", config.Model)

	return &OpenAIClient{
		config: config,
		client: client,
	}
}

// convertContent è½¬æ¢ any å†…å®¹ä¸ºå­—ç¬¦ä¸²
func convertContent(content any) string {
	if content == nil {
		return ""
	}
	if s, ok := content.(string); ok {
		return s
	}
	return fmt.Sprintf("%v", content)
}

// ChatStream æµå¼å¯¹è¯
func (c *OpenAIClient) ChatStream(ctx context.Context, req *ChatRequest) (<-chan string, <-chan error, error) {
	messages := make([]openai.ChatCompletionMessage, 0, len(req.Messages))

	// è½¬æ¢æ¶ˆæ¯æ ¼å¼
	for _, msg := range req.Messages {
		messages = append(messages, openai.ChatCompletionMessage{
			Role:    msg.Role,
			Content: convertContent(msg.Content),
		})
	}

	// æ„å»ºè¯·æ±‚
	openaiReq := openai.ChatCompletionRequest{
		Model:       c.config.Model,
		Messages:    messages,
		Temperature: 0.7,
		Stream:      true,
	}

	// æ·»åŠ å·¥å…·å®šä¹‰
	if len(req.Tools) > 0 {
		tools := make([]openai.Tool, 0, len(req.Tools))
		for _, tool := range req.Tools {
			tools = append(tools, openai.Tool{
				Type: openai.ToolTypeFunction,
				Function: &openai.FunctionDefinition{
					Name:        tool.Function.Name,
					Description: tool.Function.Description,
					Parameters:  tool.Function.Parameters,
				},
			})
		}
		openaiReq.Tools = tools
		// è®¾ç½®å·¥å…·è°ƒç”¨ç­–ç•¥ä¸º auto,è®© AI æ ¹æ®éœ€è¦å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
		openaiReq.ToolChoice = "auto"
	}

	contentCh := make(chan string, 10)
	errCh := make(chan error, 1)

	// å¼‚æ­¥å¤„ç†æµå¼å“åº”
	go func() {
		defer close(contentCh)
		defer close(errCh)

		logx.Debug("Creating chat completion stream")
		stream, err := c.client.CreateChatCompletionStream(ctx, openaiReq)
		if err != nil {
			logx.Error("Failed to create chat completion stream %v", err)
			errCh <- err
			return
		}
		defer func() { _ = stream.Close() }()

		for {
			response, err := stream.Recv()
			if errors.Is(err, io.EOF) {
				logx.Debug("Stream completed successfully")
				break
			}
			if err != nil {
				logx.Error("Stream error %v", err)
				errCh <- err
				return
			}

			// å¤„ç†æµå¼å†…å®¹
			if len(response.Choices) > 0 {
				delta := response.Choices[0].Delta.Content
				if delta != "" {
					contentCh <- delta
				}

				// å¤„ç†å·¥å…·è°ƒç”¨
				if response.Choices[0].Delta.ToolCalls != nil {
					// æµå¼æ¨¡å¼ä¸‹å·¥å…·è°ƒç”¨æ¯”è¾ƒå¤æ‚,æš‚ä¸å¤„ç†
					logx.Debug("Tool call detected in stream (not implemented in stream mode)")
				}
			}
		}
	}()

	return contentCh, errCh, nil
}

// ChatWithTools æ”¯æŒå·¥å…·è°ƒç”¨çš„å¯¹è¯(éæµå¼)
func (c *OpenAIClient) ChatWithTools(ctx context.Context, messages []Message, tools []Tool) (*ChatResponse, error) {
	openaiMessages := make([]openai.ChatCompletionMessage, 0, len(messages))
	for _, msg := range messages {
		content := convertContent(msg.Content)
		openaiMsg := openai.ChatCompletionMessage{
			Role:    msg.Role,
			Content: content,
		}

		// å¤„ç† assistant çš„å·¥å…·è°ƒç”¨
		if len(msg.ToolCalls) > 0 {
			toolCalls := make([]openai.ToolCall, 0, len(msg.ToolCalls))
			for _, tc := range msg.ToolCalls {
				toolCalls = append(toolCalls, openai.ToolCall{
					ID:   tc.ID,
					Type: openai.ToolType(tc.Type),
					Function: openai.FunctionCall{
						Name:      tc.Function.Name,
						Arguments: tc.Function.Arguments,
					},
				})
			}
			openaiMsg.ToolCalls = toolCalls
		}

		// å¤„ç† tool è§’è‰²çš„å“åº”
		if msg.ToolCallID != "" {
			openaiMsg.ToolCallID = msg.ToolCallID
		}
		if msg.Name != "" {
			openaiMsg.Name = msg.Name
		}

		openaiMessages = append(openaiMessages, openaiMsg)
	}

	// æ„å»ºè¯·æ±‚
	req := openai.ChatCompletionRequest{
		Model:       c.config.Model,
		Messages:    openaiMessages,
		Temperature: 0.7,
		Stream:      false, // å·¥å…·è°ƒç”¨æ—¶ä¸ä½¿ç”¨æµå¼
	}

	// æ·»åŠ å·¥å…·å®šä¹‰
	if len(tools) > 0 {
		openaiTools := make([]openai.Tool, 0, len(tools))
		for _, tool := range tools {
			openaiTools = append(openaiTools, openai.Tool{
				Type: openai.ToolTypeFunction,
				Function: &openai.FunctionDefinition{
					Name:        tool.Function.Name,
					Description: tool.Function.Description,
					Parameters:  tool.Function.Parameters,
				},
			})
		}
		req.Tools = openaiTools
		// è®¾ç½®å·¥å…·è°ƒç”¨ç­–ç•¥ä¸º auto,è®© AI æ ¹æ®éœ€è¦å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
		req.ToolChoice = "auto"
	}

	// è°ƒç”¨ API
	logx.Debug("Calling OpenAI API for tool execution")
	resp, err := c.client.CreateChatCompletion(ctx, req)
	if err != nil {
		logx.Error("Failed to create chat completion %v", err)
		return nil, err
	}

	if len(resp.Choices) == 0 {
		return nil, errors.New("no response choices")
	}

	choice := resp.Choices[0]

	// è½¬æ¢å“åº”
	response := &ChatResponse{
		Choices: []struct {
			Index   int `json:"index"`
			Message struct {
				Role      string     `json:"role"`
				Content   string     `json:"content"`
				ToolCalls []ToolCall `json:"tool_calls,omitempty"`
			} `json:"message"`
			Delta struct {
				Role      string     `json:"role,omitempty"`
				Content   string     `json:"content,omitempty"`
				ToolCalls []ToolCall `json:"tool_calls,omitempty"`
			} `json:"delta"`
			FinishReason string `json:"finish_reason"`
		}{
			{
				Index: choice.Index,
				Message: struct {
					Role      string     `json:"role"`
					Content   string     `json:"content"`
					ToolCalls []ToolCall `json:"tool_calls,omitempty"`
				}{
					Role:    choice.Message.Role,
					Content: choice.Message.Content,
				},
				FinishReason: string(choice.FinishReason),
			},
		},
	}

	// è½¬æ¢å·¥å…·è°ƒç”¨
	if len(choice.Message.ToolCalls) > 0 {
		toolCalls := make([]ToolCall, 0, len(choice.Message.ToolCalls))
		for _, tc := range choice.Message.ToolCalls {
			toolCalls = append(toolCalls, ToolCall{
				ID:   tc.ID,
				Type: string(tc.Type),
				Function: struct {
					Name      string `json:"name"`
					Arguments string `json:"arguments"`
				}{
					Name:      tc.Function.Name,
					Arguments: tc.Function.Arguments,
				},
			})
		}
		response.Choices[0].Message.ToolCalls = toolCalls
	}

	return response, nil
}

// Chat æ™®é€šå¯¹è¯(éæµå¼)
func (c *OpenAIClient) Chat(ctx context.Context, req *ChatRequest) (*ChatResponse, error) {
	messages := make([]openai.ChatCompletionMessage, 0, len(req.Messages))

	// è½¬æ¢æ¶ˆæ¯æ ¼å¼
	for _, msg := range req.Messages {
		messages = append(messages, openai.ChatCompletionMessage{
			Role:    msg.Role,
			Content: convertContent(msg.Content),
		})
	}

	// æ„å»ºè¯·æ±‚
	openaiReq := openai.ChatCompletionRequest{
		Model:       c.config.Model,
		Messages:    messages,
		Temperature: 0.7,
		Stream:      false,
	}

	// è°ƒç”¨ API
	resp, err := c.client.CreateChatCompletion(ctx, openaiReq)
	if err != nil {
		return nil, err
	}

	if len(resp.Choices) == 0 {
		return nil, errors.New("no response choices")
	}

	return &ChatResponse{
		Choices: []struct {
			Index   int `json:"index"`
			Message struct {
				Role      string     `json:"role"`
				Content   string     `json:"content"`
				ToolCalls []ToolCall `json:"tool_calls,omitempty"`
			} `json:"message"`
			Delta struct {
				Role      string     `json:"role,omitempty"`
				Content   string     `json:"content,omitempty"`
				ToolCalls []ToolCall `json:"tool_calls,omitempty"`
			} `json:"delta"`
			FinishReason string `json:"finish_reason"`
		}{
			{
				Index: resp.Choices[0].Index,
				Message: struct {
					Role      string     `json:"role"`
					Content   string     `json:"content"`
					ToolCalls []ToolCall `json:"tool_calls,omitempty"`
				}{
					Role:    resp.Choices[0].Message.Role,
					Content: resp.Choices[0].Message.Content,
				},
				FinishReason: string(resp.Choices[0].FinishReason),
			},
		},
	}, nil
}

// ChatWithToolsAndStream æ”¯æŒå·¥å…·è°ƒç”¨çš„æµå¼å¯¹è¯(Client æ–¹æ³•)
func (c *Client) ChatWithToolsAndStream(ctx context.Context, userMessage string) (<-chan string, error) {
	// ä¸ºäº†å‘åå…¼å®¹ï¼Œå°†å•ä¸ªæ¶ˆæ¯è½¬æ¢ä¸ºæ¶ˆæ¯åˆ—è¡¨
	messages := []Message{
		{
			Role:    "user",
			Content: userMessage,
		},
	}
	return c.ChatWithToolsAndStreamMessages(ctx, messages)
}

// ChatWithToolsAndStreamMessages ä½¿ç”¨å®Œæ•´çš„æ¶ˆæ¯å†å²ä¸ LLM å¯¹è¯
func (c *Client) ChatWithToolsAndStreamMessages(ctx context.Context, historyMessages []Message) (<-chan string, error) {
	responseCh := make(chan string, 100)

	go func() {
		defer close(responseCh)

		// æ„å»ºå®Œæ•´çš„æ¶ˆæ¯å†å²ï¼Œåœ¨æœ€å‰é¢æ·»åŠ ç³»ç»Ÿæç¤º
		messages := []Message{
			{
				Role:    "system",
				Content: c.buildSystemPrompt(),
			},
		}
		// æ·»åŠ å†å²æ¶ˆæ¯
		messages = append(messages, historyMessages...)

		// åˆ›å»º OpenAI å®¢æˆ·ç«¯
		openaiClient := NewOpenAIClient(c.config)

		// è·å–å·¥å…·åˆ—è¡¨
		tools, err := c.getMCPTools(ctx)
		if err != nil {
			logx.Warn("Failed to get MCP tools, proceeding without tools: %v", err)
			tools = nil
		}

		maxIterations := 10
		for i := 0; i < maxIterations; i++ {
			// ä½¿ç”¨æµå¼ API (æ”¯æŒå·¥å…·è°ƒç”¨)
			result, hasToolCalls, err := c.streamChatWithTools(ctx, openaiClient, messages, tools, responseCh)
			if err != nil {
				responseCh <- fmt.Sprintf("âŒ LLM è°ƒç”¨å¤±è´¥: %v", err)
				return
			}

			// å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨,è¯´æ˜å¯¹è¯ç»“æŸ
			if !hasToolCalls {
				return
			}

			// æœ‰å·¥å…·è°ƒç”¨,æ·»åŠ  assistant æ¶ˆæ¯åˆ°å†å²
			messages = append(messages, Message{
				Role:      "assistant",
				Content:   result.Content,
				ToolCalls: result.ToolCalls,
			})

			// æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
			for _, toolCall := range result.ToolCalls {
				responseCh <- fmt.Sprintf("\n> ğŸ”§ è°ƒç”¨å·¥å…·: **%s**\n", toolCall.Function.Name)

				// æ‰§è¡Œå·¥å…·è°ƒç”¨
				toolResult, err := c.executeToolCall(ctx, toolCall)
				if err != nil {
					responseCh <- fmt.Sprintf("âŒ å·¥å…·è°ƒç”¨å¤±è´¥: %v\n\n", err)
					toolResult = fmt.Sprintf("Error: %v", err)
				}

				// æ·»åŠ å·¥å…·ç»“æœåˆ°å†å²
				messages = append(messages, Message{
					Role:       "tool",
					Content:    toolResult,
					ToolCallID: toolCall.ID,
					Name:       toolCall.Function.Name,
				})

				responseCh <- "âœ… å·¥å…·æ‰§è¡Œå®Œæˆ\n\n"
			}
			// ç»§ç»­å¾ªç¯,è®© LLM å¤„ç†å·¥å…·ç»“æœ
		}

		responseCh <- "\n\nâš ï¸ è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°é™åˆ¶"
	}()

	return responseCh, nil
}

// streamChatWithTools ä½¿ç”¨æµå¼ API è¿›è¡Œå¯¹è¯(æ”¯æŒå·¥å…·è°ƒç”¨)
// è¿”å›: (ç´¯ç§¯çš„æ¶ˆæ¯å†…å®¹, æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨, é”™è¯¯)
func (c *Client) streamChatWithTools(
	ctx context.Context,
	openaiClient *OpenAIClient,
	messages []Message,
	tools []Tool,
	responseCh chan<- string,
) (*StreamResult, bool, error) {
	// æ„å»º OpenAI è¯·æ±‚
	openaiMessages := make([]openai.ChatCompletionMessage, 0, len(messages))
	for _, msg := range messages {
		content := convertContent(msg.Content)
		openaiMsg := openai.ChatCompletionMessage{
			Role:    msg.Role,
			Content: content,
		}

		// å¤„ç† assistant çš„å·¥å…·è°ƒç”¨
		if len(msg.ToolCalls) > 0 {
			toolCalls := make([]openai.ToolCall, 0, len(msg.ToolCalls))
			for _, tc := range msg.ToolCalls {
				toolCalls = append(toolCalls, openai.ToolCall{
					ID:   tc.ID,
					Type: openai.ToolType(tc.Type),
					Function: openai.FunctionCall{
						Name:      tc.Function.Name,
						Arguments: tc.Function.Arguments,
					},
				})
			}
			openaiMsg.ToolCalls = toolCalls
		}

		// å¤„ç† tool è§’è‰²çš„å“åº”
		if msg.ToolCallID != "" {
			openaiMsg.ToolCallID = msg.ToolCallID
		}
		if msg.Name != "" {
			openaiMsg.Name = msg.Name
		}

		openaiMessages = append(openaiMessages, openaiMsg)
	}

	// æ„å»ºå·¥å…·å®šä¹‰
	var openaiTools []openai.Tool
	if len(tools) > 0 {
		for _, tool := range tools {
			openaiTools = append(openaiTools, openai.Tool{
				Type: openai.ToolTypeFunction,
				Function: &openai.FunctionDefinition{
					Name:        tool.Function.Name,
					Description: tool.Function.Description,
					Parameters:  tool.Function.Parameters,
				},
			})
		}
	}

	// åˆ›å»ºæµå¼è¯·æ±‚
	openaiReq := openai.ChatCompletionRequest{
		Model:    c.config.Model,
		Messages: openaiMessages,
		Stream:   true,
	}

	if len(openaiTools) > 0 {
		openaiReq.Tools = openaiTools
		// è®¾ç½®å·¥å…·è°ƒç”¨ç­–ç•¥ä¸º auto,è®© AI æ ¹æ®éœ€è¦å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
		// å¦‚æœæƒ³å¼ºåˆ¶è°ƒç”¨å·¥å…·,å¯ä»¥æ”¹ä¸º "required"
		openaiReq.ToolChoice = "auto"
	}

	logx.Debug("Creating streaming chat completion with tools")
	stream, err := openaiClient.client.CreateChatCompletionStream(ctx, openaiReq)
	if err != nil {
		return nil, false, fmt.Errorf("failed to create stream: %w", err)
	}
	defer func() { _ = stream.Close() }()

	// ç´¯ç§¯ç»“æœ
	result := &StreamResult{
		Content:   "",
		ToolCalls: []ToolCall{},
	}

	// å·¥å…·è°ƒç”¨ç´¯ç§¯å™¨ (key: index, value: ç´¯ç§¯çš„å·¥å…·è°ƒç”¨)
	toolCallsAccumulator := make(map[int]*ToolCall)

	// å¤„ç†æµå¼å“åº”
	for {
		response, err := stream.Recv()
		if errors.Is(err, io.EOF) {
			logx.Debug("Stream completed successfully")
			break
		}
		if err != nil {
			return nil, false, fmt.Errorf("stream error: %w", err)
		}

		if len(response.Choices) == 0 {
			continue
		}

		delta := response.Choices[0].Delta

		// å¤„ç†å†…å®¹æµ
		if delta.Content != "" {
			result.Content += delta.Content
			responseCh <- delta.Content // å®æ—¶æ¨é€å†…å®¹
		}

		// å¤„ç†å·¥å…·è°ƒç”¨æµ (é€æ­¥ç´¯ç§¯)
		if len(delta.ToolCalls) > 0 {
			for _, tc := range delta.ToolCalls {
				index := tc.Index
				if index == nil {
					logx.Warn("Tool call index is nil, skipping")
					continue
				}

				// è·å–æˆ–åˆ›å»ºå·¥å…·è°ƒç”¨
				if _, exists := toolCallsAccumulator[*index]; !exists {
					newToolCall := &ToolCall{
						ID:   tc.ID,
						Type: string(tc.Type),
					}
					newToolCall.Function.Name = tc.Function.Name
					newToolCall.Function.Arguments = ""
					toolCallsAccumulator[*index] = newToolCall
				}

				// ç´¯ç§¯å‚æ•°
				if tc.Function.Arguments != "" {
					toolCallsAccumulator[*index].Function.Arguments += tc.Function.Arguments
				}

				// æ›´æ–° ID (å¦‚æœæœ‰)
				if tc.ID != "" {
					toolCallsAccumulator[*index].ID = tc.ID
				}

				// æ›´æ–°å‡½æ•°å (å¦‚æœæœ‰)
				if tc.Function.Name != "" {
					toolCallsAccumulator[*index].Function.Name = tc.Function.Name
				}
			}
		}

		// æ£€æŸ¥æ˜¯å¦ç»“æŸ
		if response.Choices[0].FinishReason != "" {
			logx.Debug("Stream finished, reason: %s", response.Choices[0].FinishReason)
			break
		}
	}

	// å°†ç´¯ç§¯çš„å·¥å…·è°ƒç”¨è½¬æ¢ä¸ºæœ‰åºåˆ—è¡¨
	if len(toolCallsAccumulator) > 0 {
		// æŒ‰ç´¢å¼•æ’åº
		indices := make([]int, 0, len(toolCallsAccumulator))
		for idx := range toolCallsAccumulator {
			indices = append(indices, idx)
		}
		sort.Ints(indices)

		// æ„å»ºå·¥å…·è°ƒç”¨åˆ—è¡¨
		for _, idx := range indices {
			result.ToolCalls = append(result.ToolCalls, *toolCallsAccumulator[idx])
		}

		logx.Info("Accumulated %d tool calls", len(result.ToolCalls))
		return result, true, nil
	}

	// æ²¡æœ‰å·¥å…·è°ƒç”¨,å¯¹è¯ç»“æŸ
	return result, false, nil
}

// StreamResult æµå¼å“åº”çš„ç´¯ç§¯ç»“æœ
type StreamResult struct {
	Content   string
	ToolCalls []ToolCall
}

// SetProxy è®¾ç½®ä»£ç†
func SetProxy(proxyURL string) error {
	if proxyURL == "" {
		return nil
	}

	_, err := url.Parse(proxyURL)
	return err
}
